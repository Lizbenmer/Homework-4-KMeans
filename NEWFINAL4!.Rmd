---
title: "NEWFINAL"
author: "Lizbeth, Nicole, Jenny, Anabel"
date: "2025-11-04"
output: pdf_document
---

```{r, echo=TRUE, warning=FALSE, message=FALSE}

#set working directory
setwd("/cloud/project")
###Importing my original data set to RStudio and calling it MCE
MCE<-read.csv("Mall_Customers_extended.csv", header=TRUE)

#to view variable names
names(MCE)


#######  Step 1: Perform a K-Means Cluster analysis   #########
# I created MCEcluster1 data frame with the only 3 variables needed
# for my cluster: annual income, spending score, age

MCEcluster<-data.frame(MCE)

MCEcluster1<-MCEcluster[ ,c("Annual.Income..k..", "Spending.Score..1.100.", "Age")]


#Now, I'm going to create a new file called MCEsv with my 3 values standardized
#so that I can begin running my clusters
MCEcluster2<-data.frame(MCEcluster1)

MCEsv <- scale(MCEcluster2)

#View first 5 observations
head(MCEsv,n=5)

###### K-Means Algorithm #######################
#The factoextra package creates clusters in R studio 

install.packages("factoextra")
library(factoextra)

install.packages("rstatix")
library(rstatix)

#To find the number of clusters needed we use the fviz_nbclust function

fviz_nbclust(MCEsv, kmeans, method="wss") + geom_vline(xintercept = 0, linetype = 2)


## I'm using 4 clusters based on the graph in my plots box
fviz_nbclust(MCEsv, kmeans, method="wss") + geom_vline(xintercept = 4, linetype = 2)




#to obtain descriptive stats on 4 clusters
set.seed(123)

km.res <- kmeans(MCEsv, 4, nstart=25)
##Per homework instructions,  do not run line 52
print(km.res)


#####  Step 2: Create the Clustering Visual  ###############

#reating a new dataset using the cbind() function to merge the subset data 
#created in step 1, and the km.res$clsuter

dd <- cbind(MCEsv, cluster=km.res$cluster)

fviz_cluster(km.res, data=dd, 
             palette = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"), 
             ellipse.type = "euclid", #Concentration ellipse
             star.plot = TRUE, #Add segments from centroids to items
             repel = TRUE, #Avoid label overplotting 
             ggtheme = theme_minimal())

#boxplot
# Step 1: Select columns for clustering
data_for_clustering <- MCE[, c("Age", "Annual.Income..k..", "Spending.Score..1.100.")]

# Step 2: Run k-means clustering 

set.seed(123)  # for reproducibility
km.res <- kmeans(data_for_clustering, centers = 3, nstart = 25)

# Step 3:  cluster assigned
km.res$cluster

# Merge cluster  with original data
MCE_clustered <- cbind(MCE, Cluster = km.res$cluster)

# Boxplot for Age across clusters
boxplot(Age ~ Cluster, data = MCE_clustered,
        main = "Distribution of Age Across Clusters",
        xlab = "Cluster", ylab = "Age",
        col = c("red", "green", "blue"))

# Boxplot for Annual Income across clusters
boxplot(Annual.Income..k.. ~ Cluster, data = MCE_clustered,
        main = "Distribution of Annual Income Across Clusters",
        xlab = "Cluster", ylab = "Annual Income (k$)",
        col = c("red", "green", "blue"))

# Boxplot for Spending Score across clusters
boxplot(Spending.Score..1.100. ~ Cluster, data = MCE_clustered,
        main = "Distribution of Spending Score Across Clusters",
        xlab = "Cluster", ylab = "Spending Score",
        col = c("red", "green", "blue"))


######################    
# descriptive tables #          
######################  


install.packages("stargazer")
library(stargazer)    

cluster1 <- subset(MCE_clustered, Cluster == 1)
cluster2 <- subset(MCE_clustered, Cluster == 2)
cluster3 <- subset(MCE_clustered, Cluster == 3)

variables <- c("Age", "Annual.Income..k..", "Spending.Score..1.100.")

stargazer(cluster1[, variables], type = "text", title = "Cluster 1 Descriptive Stats")
stargazer(cluster2[, variables], type = "text", title = "Cluster 2 Descriptive Stats")
stargazer(cluster3[, variables], type = "text", title = "Cluster 3 Descriptive Stats")

#How to do multiple linear regression in R

MallData <- read.csv("Mall_Customers_extended.csv", header=TRUE)


names(MallData)

y <- MallData$y.LoyaltyScore

x1 <- MallData$Age

x2 <- MallData$Annual.Income..k..

x3 <-MallData$Spending.Score..1.100.

str(MallData$LoyaltyScore)

y <- as.numeric(as.character(MallData$LoyaltyScore))

hist(y)




model2 <- lm(y ~ x1 + x2 + x3, data=MallData)
model2

attach(MallData)


summary(model2)
#Intercepts: There are 3 x-values that are independent: these are x1 is the
#intercept for Age. x2 is the intercept for Annual Income K and x3 is the
#intercept for Spending Score.The estimated Beta for x1 is -0.08975, x2 is
#-0.01474 and x3 is 0.98109. There are 3 different variables with different 
#outcomes.Age, Annual Income K and Spending Score are all continuous variables. 
#For every unit in x1 (Age), there is a decrease of -0.0875 for the outcome 
#(LoyaltyScore). Applies for x2, there is a decrease of -0.01474 for the outcome
#and for x3, there is an increase of 0.98108 for the outcome. 

#Based on the summary, x1 and x2 are not significant. They do not contain the
#asterick. In this multiple Linear Regression ,x3 significantly predicts the 
#outcome. For every unit in x3, the expected outcome increases by 0.98109 
#which is adjusting for x1 and x2.

#Mulitple R-Squared: This is used for Multiple Linear Regression to avoid bias 
#when trying to find associations of x varibles with outcome. When adding more 
#variables R, usually increases, therefore we avoid using Multiple R-squared in 
#this situation. Thus, they both provide the proportion of variability that 
#the x's will explain on the variance of the outcome. It is ideal for it to be 
#high, it should be close to 1. This would mean that all of systemcatic 
#variances are being accounted for, in this model, it is high of 0.894. 

#F-Statistics: The F-statistics is more than 1, it is 560.7, this means that
#p-value is significant as well. When F-statistics is more than 1, it means that
#the systematic variances are being accounted by these x variables and they 
#outweigh the unsystematic variances. 

#p-Value: This would be considered a good model because it is less than 0.05,
#shows that it is statistically significant. 

#Income does not predict more Loyalty stronger than other clusters. The cluster
#that show to have a stronger Loyalty is x3 (Spending Score) with a positive 
#beta of 0.98109. The other betas for x1 and x2 variable show decrease with 
#association of outcome.




#Assessing fit of model
plot(model2)





```

